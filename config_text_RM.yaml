# Model arguments
model_name_or_path: Qwen/Qwen2.5-1.5B-Instruct
model_revision: main
torch_dtype: bfloat16
attn_implementation: flash_attention_2

# Data training arguments
dataset_name: openai/gsm8k
dataset_config: main

system_prompt: "A conversation between user and assistant. The user asks a question, and the assistant solves it. The assistant first thinks about the reasoning process in the mind and then provides the user with the answer summarizing the reasoning results. The reasoning process and answer are enclosed within <think> </think> and <answer> </answer> tags, respectively, i.e., <think> reasoning process here </think> <answer> answer here </answer>."

# GRPO trainer config
bf16: true
use_vllm: true
vllm_device: auto
vllm_gpu_memory_utilization: 0.7
vllm_dtype: bfloat16
max_grad_norm: 1.0
gradient_accumulation_steps: 9
gradient_checkpointing: true
gradient_checkpointing_kwargs:
  use_reentrant: false
hub_model_id: Qwen-2.5-1.5B-Instruct-gsm8k
hub_strategy: every_save
learning_rate: 3e-07
lr_scheduler_type: linear
log_completions: true
log_level: info
logging_first_step: true
logging_steps: 1
logging_strategy: steps
temperature: 1.0
beta: 0.04
max_prompt_length: 512
max_completion_length: 512
max_steps: -1
num_generations: 16
num_train_epochs: 10
overwrite_output_dir: true
per_device_eval_batch_size: 16
per_device_train_batch_size: 16
push_to_hub: false
wandb_project: text-reasoning
report_to:
- wandb
reward_funcs:
- format
- gen_orm_reward
- gen_prm_reward
reward_weights:
- 1.0
- 1.0
- 1.0
save_strategy: "steps"
save_steps: 100
save_only_model: true
seed: 42
